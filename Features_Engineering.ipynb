{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Features Engineering"
      ],
      "metadata": {
        "id": "sRPFYQAZVr9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1.What is a parameter?\n",
        "\n",
        "\n",
        " - a parameter is a configuration variable internal to the model whose value can be estimated from the data. These are the values that the learning algorithm optimizes during training, such as the weights and biases in a neural network or the coefficients in a linear regression model.\n",
        "\n",
        "\n",
        "\n",
        "#Q2.What is correlation? What does negative correlation mean?\n",
        "\n",
        "\n",
        " - Correlation refers to the relationship between variables\n",
        "\n",
        "- Negative correlation means that as one variable increases, the other variable decreases.\n",
        "\n",
        "\n",
        "#Q3.Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "\n",
        "- Machine learning is a field of artificial intelligence that uses statistical techniques to give computer systems the ability to \"learn\" from data, without being explicitly programmed. It involves developing algorithms that can identify patterns in data and make predictions or decisions based on that learning.\n",
        "\n",
        "\n",
        "The main components in Machine Learning typically include:\n",
        "\n",
        "- Data: The raw information from which the model learns.\n",
        "- Features: The specific, measurable attributes or characteristics of the data.\n",
        "- Model: The algorithm or mathematical representation that learns patterns from the data.\n",
        "- Loss Function (or Cost Function): A function that measures the discrepancy between the model's predictions and the actual values.\n",
        "- Optimizer: An algorithm used to minimize the loss function by adjusting the model's parameters.\n",
        "- Evaluation Metrics: Measures used to assess the performance of the model (e.g., accuracy, precision, recall, F1-score, MSE).\n",
        "\n",
        "\n",
        "#Q4.How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "- A loss value quantifies the error or discrepancy between the predicted output of a machine learning model and the actual output. A lower loss value indicates that the model's predictions are closer to the actual values, suggesting a better-performing model. Conversely, a high loss value indicates poor performance, meaning the model's predictions are far from the true values. The goal during model training is to minimize this loss value.\n",
        "\n",
        "#Q5.What are continuous and categorical variables?\n",
        "\n",
        "- Continuous variables are variables that can take on any value within a given range.  Categorical variables are variables that can take on a limited number of distinct values, often representing categories or labels.\n",
        "\n",
        "\n",
        "#Q6.How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "- Data encoding is used to handle categorical variables in Machine Learning.\n",
        "\n",
        "#What are the common techniques?\n",
        "\n",
        "Common techniques for handling categorical variables include:\n",
        "\n",
        "- One-Hot Encoding: Creates new binary (0 or 1) columns for each category.\n",
        "- Label Encoding: Assigns a unique integer to each category.\n",
        "- Ordinal Encoding: Similar to label encoding but used when there's a meaningful order to the categories.\n",
        "- Binary Encoding: Converts categories to binary code.\n",
        "- Target Encoding (or Mean Encoding): Replaces each category with the mean of the target variable for that category.\n",
        "\n",
        "#Q7.What do you mean by training and testing a dataset?\n",
        "\n",
        " - Testing a dataset involves using a separate portion of the data to evaluate the model's performance on unseen data.\n",
        "\n",
        "\n",
        "\n",
        "#Q8.What is sklearn.preprocessing?\n",
        "\n",
        "- sklearn.preprocessing is a module in scikit-learn that provides utilities for data preprocessing, such as scaling and encoding\n",
        "\n",
        "\n",
        "#Q9.What is a Test set?\n",
        "\n",
        "- A test set is a portion of the data used to evaluate the performance of a machine learning model after it has been trained.\n",
        "\n",
        "\n",
        "#Q10.How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "\n",
        " - Data is split for model fitting (training and testing) in Python by dividing the dataset into training and testing subsets.  This is commonly done using functions like train_test_split from sklearn.model_selection.\n",
        "\n",
        "#How do you approach a Machine Learning problem?\n",
        "\n",
        " A typical approach to a Machine Learning problem involves several steps:\n",
        "\n",
        "- Problem Definition: Clearly define the objective and desired outcome.\n",
        "- Data Collection: Gather relevant data.\n",
        "- Data Preprocessing/Cleaning: Handle missing values, outliers, and inconsistencies.\n",
        "- Exploratory Data Analysis (EDA): Understand the data's characteristics and relationships.\n",
        "- Feature Engineering: Create new features or transform existing ones to improve model performance.\n",
        "- Model Selection: Choose an appropriate machine learning algorithm.\n",
        "- Model Training: Train the model on the training data.\n",
        "- Model Evaluation: Assess the model's performance on the test data.\n",
        "- Hyperparameter Tuning: Optimize model parameters for better performance.\n",
        "- Deployment: Integrate the trained model into an application.\n",
        "- Monitoring and Maintenance: Continuously monitor and update the model.\n",
        "\n",
        "\n",
        "#Q11.Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "\n",
        "- EDA (Exploratory Data Analysis) is performed before fitting a model to the data to understand the data's characteristics, identify patterns, and detect anomalies.  It helps in gaining insights, formulating hypotheses, identifying potential issues (like outliers or missing values), and guiding subsequent feature engineering and model selection steps.\n",
        "\n",
        "\n",
        "#Q12.What is correlation?\n",
        "\n",
        "- Correlation refers to the relationship between variables.\n",
        "\n",
        "#Q13.What does negative correlation mean?\n",
        "\n",
        "\n",
        "- Negative correlation means that as one variable increases, the other variable decreases.\n",
        "\n",
        "\n",
        "#Q14.How can you find correlation between variables in Python?\n",
        "\n",
        "\n",
        "- You can find correlation between variables in Python using libraries like pandas and numpy. The corr() method on a pandas DataFrame can compute pairwise correlation between columns.\n",
        "\n",
        "For example:"
      ],
      "metadata": {
        "id": "n662y_v5XjKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = {'A': [1, 2, 3, 4, 5],\n",
        "        'B': [5, 4, 3, 2, 1],\n",
        "        'C': [1, 1, 2, 2, 3]}\n",
        "df = pd.DataFrame(data)\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyzFyMOzq_sI",
        "outputId": "7e3f7a05-5679-42e3-f89b-b69a8901686b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          A         B         C\n",
            "A  1.000000 -1.000000  0.944911\n",
            "B -1.000000  1.000000 -0.944911\n",
            "C  0.944911 -0.944911  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q15.What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "- Correlation refers to the relationship between variables.\n",
        "- Causation, on the other hand, means that one event is the result of the occurrence of the other event; there is a causal relationship between them.\n",
        "\n",
        "Difference: Correlation implies a relationship, but not necessarily that one causes the other. Causation implies a direct cause-and-effect relationship.\n",
        "\n",
        "Example:\n",
        "\n",
        "\n",
        "- Correlation without Causation: Ice cream sales and drowning incidents often increase during the summer months. They are correlated (they tend to rise and fall together), but ice cream sales do not cause drownings. Both are influenced by a third factor: warmer weather leading to more people buying ice cream and more people swimming.\n",
        "- Causation: If you push a domino, it falls. Pushing the domino causes it to fall.\n",
        "\n",
        "\n",
        "#Q16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "\n",
        "- An optimizer is an algorithm or function used to adjust the parameters of a machine learning model during training to minimize the loss function.\n",
        "\n",
        "\n",
        "- Gradient Descent (GD): Updates model parameters in the direction opposite to the gradient of the loss function with respect to the parameters. It computes the gradient using the entire training dataset for each update.\n",
        "  - Example: In a simple linear regression, GD would iteratively adjust the slope and intercept to minimize the sum of squared errors.\n",
        "- Stochastic Gradient Descent (SGD): Similar to GD but updates parameters using the gradient of a single training example (or a small mini-batch) at a time. This makes it faster for large datasets.\n",
        "  - Example: In training a neural network, SGD might update weights after processing each image in a batch, rather than waiting for all images.\n",
        "- Mini-Batch Gradient Descent: A compromise between GD and SGD, updating parameters using a small batch of training examples. This offers a balance between computational efficiency and stability.\n",
        "  - Example: Updating weights in a deep learning model using batches of 32 or 64 samples at a time.\n",
        "- Adam (Adaptive Moment Estimation): An adaptive learning rate optimization algorithm that computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. It's widely used due to its efficiency and good performance.\n",
        "  - Example: Often the default optimizer for deep neural networks in frameworks like TensorFlow and PyTorch, as it generally converges quickly.\n",
        "- RMSprop (Root Mean Square Propagation): An adaptive learning rate optimizer that divides the learning rate by an exponentially decaying average of squared gradients. It helps in dealing with vanishing/exploding gradients.\n",
        "  - Example: Useful in recurrent neural networks (RNNs) where gradients can be problematic.\n",
        "\n",
        "\n",
        "#Q17.What is sklearn.linear_model ?\n",
        "\n",
        "- sklearn.linear_model is a module within the scikit-learn library in Python that provides various linear models for regression and classification tasks. These models assume a linear relationship between the input features and the output variable. Examples include Linear Regression, Logistic Regression, Ridge, Lasso, etc.\n",
        "\n",
        "\n",
        "#Q18.What does model.fit() do? What arguments must be given?\n",
        "\n",
        "\n",
        "- model.fit() is used to train a machine learning model using the provided training data. It essentially learns the patterns and relationships within the input data and adjusts the model's internal parameters to minimize the error.\n",
        "\n",
        "- The arguments that must be given to model.fit() are the features (input data), typically denoted as X, and the target variable (output data), typically denoted as y.\n",
        "\n",
        " - X: The training data (features), usually a 2D array or DataFrame where rows are samples and columns are features.\n",
        " - y: The target values (labels), usually a 1D array or Series corresponding to the X samples.\n",
        "\n",
        "\n",
        "#Q19.What does model.predict() do? What arguments must be given?\n",
        "\n",
        "- model.predict() is used to make predictions using a trained machine learning model. It takes new, unseen input data and uses the patterns learned during training to generate output predictions.\n",
        "\n",
        "- The arguments that must be given are the new data points for which predictions are desired, typically denoted as X_new or X_test. This input data should have the same number of features and the same structure as the training data.\n",
        "\n",
        "\n",
        "#Q20.What are continuous and categorical variables?\n",
        "\n",
        "- Continuous variables are variables that can take on any value within a given range.\n",
        "- Categorical variables are variables that can take on a limited number of distinct values, often representing categories or labels.\n",
        "\n",
        "\n",
        "#Q21.What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "- Feature scaling is the process of transforming the range of independent variables or features of the data. It helps in Machine Learning by ensuring that all features contribute equally to the model's performance and prevents features with larger values from dominating those with smaller values.\n",
        "\n",
        "\n",
        "#Q22.How do we perform scaling in Python?\n",
        "\n",
        "- Scaling in Python is typically performed using modules like sklearn.preprocessing. Common classes for scaling include:\n",
        "\n",
        " - StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
        " - MinMaxScaler: Scales features to a given range, usually between 0 and 1.\n",
        " - RobustScaler: Scales features using statistics that are robust to outliers (e.g., median and interquartile range).\n",
        " - Normalizer: Scales individual samples to have unit norm."
      ],
      "metadata": {
        "id": "-E3FhIWBrKIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example using StandardScaler:\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[1.0, 100.0],\n",
        "                 [2.0, 150.0],\n",
        "                 [3.0, 120.0]])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLY71QY3v69Y",
        "outputId": "3ca61918-571c-435a-96b6-5cc8efacf71b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.13554995]\n",
            " [ 0.          1.29777137]\n",
            " [ 1.22474487 -0.16222142]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q23.What is sklearn.preprocessing?\n",
        "\n",
        "- sklearn.preprocessing is a module in scikit-learn that provides utilities for data preprocessing, such as scaling and encoding.\n",
        "\n",
        "\n",
        "#Q24.How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "- Data is split for model fitting (training and testing) in Python by dividing the dataset into training and testing subsets.  This is commonly done using the train_test_split function from sklearn.model_selection."
      ],
      "metadata": {
        "id": "FBbhg7G7wHZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np # Import numpy for creating sample data\n",
        "\n",
        "# --- Create a sample DataFrame for demonstration ---\n",
        "# In a real scenario, you would load your data here, e.g., df = pd.read_csv('your_data.csv')\n",
        "data = {\n",
        "    'Feature1': np.random.rand(100),\n",
        "    'Feature2': np.random.rand(100) * 10,\n",
        "    'Feature3': np.random.randint(0, 5, 100),\n",
        "    'Target': np.random.randint(0, 2, 100) # Binary classification target\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# --- Define your features (X) and target (y) ---\n",
        "# X will contain all columns except the 'Target' column\n",
        "X = df.drop('Target', axis=1) # Features (independent variables)\n",
        "# y will contain only the 'Target' column\n",
        "y = df['Target']             # Target (dependent variable)\n",
        "\n",
        "# --- Split the data into training and testing sets ---\n",
        "# test_size=0.2 means 20% of the data will be used for testing, and 80% for training.\n",
        "# random_state=42 is a seed for the random number generator, ensuring\n",
        "# that your split is the same every time you run the code.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Print the shapes of the resulting datasets to verify the split ---\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n",
        "\n",
        "print(\"\\nFirst 5 rows of X_train:\")\n",
        "print(X_train.head())\n",
        "\n",
        "print(\"\\nFirst 5 rows of y_train:\")\n",
        "print(y_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJCJ8mYWwmfK",
        "outputId": "599e2044-c1c5-4058-938d-607182ae8294"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (80, 3)\n",
            "Shape of X_test: (20, 3)\n",
            "Shape of y_train: (80,)\n",
            "Shape of y_test: (20,)\n",
            "\n",
            "First 5 rows of X_train:\n",
            "    Feature1  Feature2  Feature3\n",
            "55  0.116867  3.464011         2\n",
            "88  0.751138  1.539379         4\n",
            "26  0.380566  1.241510         0\n",
            "42  0.566082  1.044067         0\n",
            "69  0.696873  8.739956         2\n",
            "\n",
            "First 5 rows of y_train:\n",
            "55    1\n",
            "88    0\n",
            "26    0\n",
            "42    1\n",
            "69    1\n",
            "Name: Target, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QExplain data encoding?\n",
        "- Data encoding is the process of converting categorical data into a numerical format that can be understood by machine learning algorithms. Machine learning models typically require numerical input, so categorical variables (like \"Red\", \"Blue\", \"Green\" or \"High\", \"Medium\", \"Low\") need to be transformed into numbers before they can be used effectively. This transformation allows algorithms to perform mathematical operations and learn from these features.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rs7OkTcfxv-2"
      }
    }
  ]
}